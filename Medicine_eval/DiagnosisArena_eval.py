# diagnosis_arena_auto_test.py
#!/usr/bin/env python3
"""
DiagnosisArena è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬
è‡ªåŠ¨è°ƒç”¨é¡¹ç›®å†…çš„è„šæœ¬æ¥æµ‹è¯•ä¸¤ç§æ¨¡å¼
"""

import os
import sys
import subprocess
import time
import json
import argparse
from pathlib import Path
from datetime import datetime

# é¡¹ç›®é…ç½®
PROJECT_CONFIG = {
    "api_key": "sk-OqIPE7A0rEMX8Rwt5NFrxB5TKAruSRGQVw7dUPRh78QpwGUi",
    "base_url": "http://123.129.219.111:3000/v1",
    "model": "deepseek-r1",  # ä½¿ç”¨ä½ APIæ”¯æŒçš„æ¨¡å‹
    "project_dir": "DiagnosisArena/code",  # é¡¹ç›®è„šæœ¬ç›®å½•
    "hf_data_path": "SII-SPIRAL-MED/DiagnosisArena",
    "test_size": 10,  # æµ‹è¯•æ ·æœ¬æ•°ï¼ˆå¯é€‰ï¼‰
    "folk_nums": 4,  # å¹¶å‘æ•°ï¼Œæ ¹æ®APIé™åˆ¶è°ƒæ•´
    "test_timestamp": datetime.now().strftime("%Y%m%d_%H%M%S")
}

class DiagnosisArenaAutoTester:
    """è‡ªåŠ¨åŒ–æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.project_dir = Path(self.config["project_dir"])
        self.results_dir = Path("test_results")
        self.results_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•ç»“æœæ–‡ä»¶è·¯å¾„
        self.test_id = f"test_{self.config['test_timestamp']}"
        self.open_ended_files = {
            "answers": self.results_dir / f"{self.test_id}_open_answers.jsonl",
            "evaluated": self.results_dir / f"{self.test_id}_open_evaluated.jsonl",
            "metrics": self.results_dir / f"{self.test_id}_open_metrics.txt"
        }
        self.mcq_files = {
            "answers": self.results_dir / f"{self.test_id}_mcq_answers.jsonl",
            "metrics": self.results_dir / f"{self.test_id}_mcq_metrics.txt"
        }
        
    def print_header(self, title):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "=" * 70)
        print(f"ğŸ”¬ {title}")
        print("=" * 70)
    
    def print_step(self, step_num, description):
        """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
        print(f"\nğŸ“‹ æ­¥éª¤ {step_num}: {description}")
        print("-" * 50)
    
    def run_command(self, command, description, output_file=None):
        """è¿è¡Œå‘½ä»¤å¹¶æ•è·è¾“å‡º"""
        print(f"  æ‰§è¡Œ: {description}")
        print(f"  å‘½ä»¤: {' '.join(command)}")
        
        try:
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    result = subprocess.run(
                        command,
                        capture_output=True,
                        text=True,
                        check=False
                    )
                    # å†™å…¥è¾“å‡º
                    f.write("STDOUT:\n" + result.stdout)
                    f.write("\n\nSTDERR:\n" + result.stderr)
                    f.write(f"\n\nè¿”å›ç : {result.returncode}")
            else:
                result = subprocess.run(
                    command,
                    capture_output=True,
                    text=True,
                    check=False
                )
            
            if result.returncode != 0:
                print(f"  âš ï¸  è­¦å‘Š: è¿”å›ç  {result.returncode}")
                if result.stderr:
                    print(f"  é”™è¯¯è¾“å‡º:\n{result.stderr[:500]}...")
            else:
                print("  âœ… å®Œæˆ")
                
            return result
            
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            return None
    
    def modify_script_for_testing(self, script_path, test_size=None):
        """ä¿®æ”¹è„šæœ¬ä»¥é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°"""
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŸ¥æ‰¾å¹¶å–æ¶ˆæ³¨é‡Šæµ‹è¯•è¡Œ
            test_line = '# input_datas = input_datas.select(range(10))'
            if test_line in content:
                if test_size:
                    new_line = f'input_datas = input_datas.select(range({test_size}))'
                else:
                    new_line = 'input_datas = input_datas.select(range(10))'
                content = content.replace(test_line, new_line)
                print(f"  ğŸ”§ ä¿®æ”¹è„šæœ¬ä»¥æµ‹è¯• {test_size if test_size else 10} ä¸ªæ ·æœ¬")
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_path = script_path.with_suffix('.temp.py')
            with open(temp_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return temp_path
            
        except Exception as e:
            print(f"  âš ï¸  æ— æ³•ä¿®æ”¹è„šæœ¬: {e}")
            return script_path
    
    def test_open_ended_mode(self):
        """æµ‹è¯•æ¨¡å¼ä¸€ï¼šå¼€æ”¾å¼è¯Šæ–­"""
        self.print_header("æ¨¡å¼ä¸€æµ‹è¯•ï¼šå¼€æ”¾å¼è¯Šæ–­ (Top-5è¯Šæ–­)")
        
        # æ­¥éª¤1: ç”Ÿæˆè¯Šæ–­ç»“æœ
        self.print_step(1, "ç”Ÿæˆè¯Šæ–­ç»“æœ")
        
        # ä¿®æ”¹inference.pyä»¥é™åˆ¶æµ‹è¯•æ ·æœ¬
        inference_script = self.project_dir / "inference.py"
        temp_inference = self.modify_script_for_testing(inference_script, self.config["test_size"])
        
        cmd = [
            "python", str(temp_inference),
            "--hf_data_path", self.config["hf_data_path"],
            "--model_name", self.config["model"],
            "--output_path", str(self.open_ended_files["answers"]),
            "--api_key", self.config["api_key"],
            "--base_url", self.config["base_url"],
            "--folk_nums", str(self.config["folk_nums"])
        ]
        
        result = self.run_command(cmd, "è¿è¡Œæ¨ç†è„šæœ¬", 
                                 self.open_ended_files["metrics"].with_suffix(".inference.log"))
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_inference != inference_script and temp_inference.exists():
            temp_inference.unlink()
        
        if not result or result.returncode != 0:
            print("  âŒ æ¨ç†æ­¥éª¤å¤±è´¥ï¼Œè·³è¿‡åç»­æ­¥éª¤")
            return False
        
        # æ£€æŸ¥ç”Ÿæˆäº†å¤šå°‘æ¡ç»“æœ
        if self.open_ended_files["answers"].exists():
            with open(self.open_ended_files["answers"], 'r', encoding='utf-8') as f:
                lines = f.readlines()
                print(f"  ğŸ“Š ç”Ÿæˆäº† {len(lines)} æ¡è¯Šæ–­ç»“æœ")
        
        # æ­¥éª¤2: ä½¿ç”¨LLMè£åˆ¤è¯„åˆ†
        self.print_step(2, "ä½¿ç”¨LLMè£åˆ¤è¯„åˆ†")
        
        cmd = [
            "python", str(self.project_dir / "evaluation.py"),
            "--input_path", str(self.open_ended_files["answers"]),
            "--output_path", str(self.open_ended_files["evaluated"]),
            "--model_name", self.config["model"],
            "--api_key", self.config["api_key"],
            "--base_url", self.config["base_url"],
            "--folk_nums", str(self.config["folk_nums"])
        ]
        
        result = self.run_command(cmd, "è¿è¡Œè¯„ä¼°è„šæœ¬",
                                 self.open_ended_files["metrics"].with_suffix(".evaluation.log"))
        
        if not result or result.returncode != 0:
            print("  âš ï¸  è¯„ä¼°æ­¥éª¤å¯èƒ½æœªå®Œå…¨å®Œæˆ")
        
        # æ­¥éª¤3: è®¡ç®—æŒ‡æ ‡
        self.print_step(3, "è®¡ç®—Top-kæŒ‡æ ‡")
        
        cmd = [
            "python", str(self.project_dir / "metric.py"),
            "--model_name", self.config["model"],
            "--metric_path", str(self.open_ended_files["evaluated"])
        ]
        
        result = self.run_command(cmd, "è¿è¡ŒæŒ‡æ ‡è®¡ç®—è„šæœ¬")
        
        if result and result.stdout:
            # ä¿å­˜æŒ‡æ ‡ç»“æœ
            with open(self.open_ended_files["metrics"], 'w', encoding='utf-8') as f:
                f.write(f"å¼€æ”¾å¼è¯Šæ–­æµ‹è¯•ç»“æœ - {self.test_id}\n")
                f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now()}\n")
                f.write(f"æ¨¡å‹: {self.config['model']}\n")
                f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {self.config['test_size']}\n")
                f.write("=" * 50 + "\n\n")
                f.write(result.stdout)
            
            print(f"\nğŸ“ˆ æŒ‡æ ‡ç»“æœå·²ä¿å­˜åˆ°: {self.open_ended_files['metrics']}")
            print("\næŒ‡æ ‡æ‘˜è¦:")
            print(result.stdout)
        
        return True
    
    def test_mcq_mode(self):
        """æµ‹è¯•æ¨¡å¼äºŒï¼šå¤šé€‰é¢˜"""
        self.print_header("æ¨¡å¼äºŒæµ‹è¯•ï¼šå¤šé€‰é¢˜ (å››é€‰ä¸€)")
        
        # æ­¥éª¤1: ç”Ÿæˆç­”æ¡ˆ
        self.print_step(1, "ç”Ÿæˆå¤šé€‰é¢˜ç­”æ¡ˆ")
        
        # ä¿®æ”¹inference_mcq.pyä»¥é™åˆ¶æµ‹è¯•æ ·æœ¬
        inference_mcq_script = self.project_dir / "inference_mcq.py"
        temp_inference_mcq = self.modify_script_for_testing(inference_mcq_script, self.config["test_size"])
        
        cmd = [
            "python", str(temp_inference_mcq),
            "--hf_data_path", self.config["hf_data_path"],
            "--model_name", self.config["model"],
            "--output_path", str(self.mcq_files["answers"]),
            "--api_key", self.config["api_key"],
            "--base_url", self.config["base_url"],
            "--folk_nums", str(self.config["folk_nums"])
        ]
        
        result = self.run_command(cmd, "è¿è¡Œå¤šé€‰é¢˜æ¨ç†è„šæœ¬",
                                 self.mcq_files["metrics"].with_suffix(".inference.log"))
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_inference_mcq != inference_mcq_script and temp_inference_mcq.exists():
            temp_inference_mcq.unlink()
        
        if not result or result.returncode != 0:
            print("  âŒ å¤šé€‰é¢˜æ¨ç†æ­¥éª¤å¤±è´¥ï¼Œè·³è¿‡åç»­æ­¥éª¤")
            return False
        
        # æ£€æŸ¥ç”Ÿæˆäº†å¤šå°‘æ¡ç»“æœ
        if self.mcq_files["answers"].exists():
            with open(self.mcq_files["answers"], 'r', encoding='utf-8') as f:
                lines = f.readlines()
                print(f"  ğŸ“Š ç”Ÿæˆäº† {len(lines)} æ¡å¤šé€‰é¢˜ç­”æ¡ˆ")
        
        # æ­¥éª¤2: è®¡ç®—å‡†ç¡®ç‡
        self.print_step(2, "è®¡ç®—å‡†ç¡®ç‡")
        
        cmd = [
            "python", str(self.project_dir / "metric_mcq.py"),
            "--model_name", self.config["model"],
            "--metric_path", str(self.mcq_files["answers"])
        ]
        
        result = self.run_command(cmd, "è¿è¡Œå¤šé€‰é¢˜æŒ‡æ ‡è®¡ç®—è„šæœ¬")
        
        if result and result.stdout:
            # ä¿å­˜æŒ‡æ ‡ç»“æœ
            with open(self.mcq_files["metrics"], 'w', encoding='utf-8') as f:
                f.write(f"å¤šé€‰é¢˜æµ‹è¯•ç»“æœ - {self.test_id}\n")
                f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now()}\n")
                f.write(f"æ¨¡å‹: {self.config['model']}\n")
                f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {self.config['test_size']}\n")
                f.write("=" * 50 + "\n\n")
                f.write(result.stdout)
            
            print(f"\nğŸ“ˆ æŒ‡æ ‡ç»“æœå·²ä¿å­˜åˆ°: {self.mcq_files['metrics']}")
            print("\næŒ‡æ ‡æ‘˜è¦:")
            print(result.stdout)
        
        return True
    
    def generate_summary_report(self):
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦æŠ¥å‘Š"""
        summary_file = self.results_dir / f"{self.test_id}_summary.md"
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        self.results_dir.mkdir(exist_ok=True)
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"# DiagnosisArena æµ‹è¯•æ‘˜è¦\n\n")
            f.write(f"**æµ‹è¯•ID**: {self.test_id}\n")
            f.write(f"**æµ‹è¯•æ—¶é—´**: {datetime.now()}\n")
            f.write(f"**æ¨¡å‹**: {self.config['model']}\n")
            f.write(f"**APIç«¯ç‚¹**: {self.config['base_url']}\n")
            f.write(f"**æµ‹è¯•æ ·æœ¬æ•°**: {self.config['test_size']}\n")
            f.write(f"**å¹¶å‘æ•°**: {self.config['folk_nums']}\n\n")
            
            f.write("## æ–‡ä»¶è¾“å‡º\n\n")
            
            # å¼€æ”¾å¼è¯Šæ–­ç»“æœ
            f.write("### æ¨¡å¼ä¸€ï¼šå¼€æ”¾å¼è¯Šæ–­\n")
            f.write(f"- è¯Šæ–­ç»“æœ: `{self.open_ended_files['answers'].name}`\n")
            f.write(f"- è¯„ä¼°ç»“æœ: `{self.open_ended_files['evaluated'].name}`\n")
            f.write(f"- æŒ‡æ ‡æ–‡ä»¶: `{self.open_ended_files['metrics'].name}`\n\n")
            
            # å¤šé€‰é¢˜ç»“æœ
            f.write("### æ¨¡å¼äºŒï¼šå¤šé€‰é¢˜\n")
            f.write(f"- ç­”æ¡ˆæ–‡ä»¶: `{self.mcq_files['answers'].name}`\n")
            f.write(f"- æŒ‡æ ‡æ–‡ä»¶: `{self.mcq_files['metrics'].name}`\n\n")
            
            f.write("## å¿«é€ŸæŸ¥çœ‹ç»“æœ\n\n")
            f.write("```bash\n")
            f.write("# æŸ¥çœ‹å¼€æ”¾å¼è¯Šæ–­æŒ‡æ ‡\n")
            # ä¿®å¤ï¼šä½¿ç”¨ç›¸å¯¹äºå½“å‰ç›®å½•çš„è·¯å¾„
            rel_open_path = self.open_ended_files['metrics'].relative_to(Path.cwd())
            f.write(f"cat {rel_open_path}\n\n")
            
            f.write("# æŸ¥çœ‹å¤šé€‰é¢˜æŒ‡æ ‡\n")
            rel_mcq_path = self.mcq_files['metrics'].relative_to(Path.cwd())
            f.write(f"cat {rel_mcq_path}\n\n")
            
            f.write("# æŸ¥çœ‹æ‰€æœ‰æµ‹è¯•æ–‡ä»¶\n")
            f.write(f"ls -la {self.results_dir.name}/\n")
            f.write("```\n")
        
        print(f"\nğŸ“‹ æµ‹è¯•æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
        return summary_file
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ DiagnosisArena è‡ªåŠ¨åŒ–æµ‹è¯•å¼€å§‹")
        print(f"ğŸ“ é¡¹ç›®ç›®å½•: {self.project_dir.absolute()}")
        print(f"ğŸ’¾ ç»“æœç›®å½•: {self.results_dir.absolute()}")
        print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹: {self.config['model']}")
        print(f"ğŸ”— APIç«¯ç‚¹: {self.config['base_url']}")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬: {self.config['test_size']} ä¸ª\n")
        
        start_time = time.time()
        
        # æ£€æŸ¥é¡¹ç›®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not self.project_dir.exists():
            print(f"âŒ é”™è¯¯: é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {self.project_dir}")
            print("è¯·ç¡®ä¿é¡¹ç›®ç»“æ„ä¸º:")
            print("  /å½“å‰ç›®å½•")
            print("  â”œâ”€â”€ diagnosis_arena_auto_test.py  (æœ¬è„šæœ¬)")
            print("  â””â”€â”€ DiagnosisArena/")
            print("      â””â”€â”€ code/  (é¡¹ç›®è„šæœ¬ç›®å½•)")
            return False
        
        # æ£€æŸ¥å¿…è¦è„šæœ¬
        required_scripts = ["inference.py", "evaluation.py", "metric.py", "inference_mcq.py", "metric_mcq.py"]
        missing = []
        for script in required_scripts:
            if not (self.project_dir / script).exists():
                missing.append(script)
        
        if missing:
            print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„è„šæœ¬æ–‡ä»¶: {missing}")
            return False
        
        # è¿è¡Œæµ‹è¯•
        test_results = {}
        
        try:
            # æµ‹è¯•æ¨¡å¼ä¸€
            test_results["open_ended"] = self.test_open_ended_mode()
            
            # ç­‰å¾…ä¸€ä¸‹é¿å…APIè¿‡è½½
            time.sleep(2)
            
            # æµ‹è¯•æ¨¡å¼äºŒ
            test_results["mcq"] = self.test_mcq_mode()
            
            # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            summary_file = self.generate_summary_report()
            
            total_time = time.time() - start_time
            
            print("\n" + "=" * 70)
            print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
            print("=" * 70)
            print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f} ç§’")
            print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir.absolute()}")
            
            if summary_file.exists():
                print(f"ğŸ“‹ æµ‹è¯•æ‘˜è¦: {summary_file.relative_to(Path.cwd())}")
            
            print("\nğŸ“Š æµ‹è¯•ç»“æœæ–‡ä»¶:")
            for file in self.results_dir.glob(f"{self.test_id}_*"):
                print(f"  - {file.name}")
            
            print("\nğŸ” æŸ¥çœ‹ç»“æœ:")
            print(f"  cat {self.results_dir.name}/{self.test_id}_summary.md")
            
            return True
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            return False
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='DiagnosisArena è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬')
    parser.add_argument('--project_dir', type=str, default="DiagnosisArena/code",
                       help='é¡¹ç›®è„šæœ¬ç›®å½•è·¯å¾„')
    parser.add_argument('--model', type=str, default="deepseek-r1",
                       help='è¦æµ‹è¯•çš„æ¨¡å‹åç§°')
    parser.add_argument('--test_size', type=int, default=10,
                       help='æµ‹è¯•æ ·æœ¬æ•°é‡')
    parser.add_argument('--folk_nums', type=int, default=4,
                       help='å¹¶å‘è¯·æ±‚æ•°')
    parser.add_argument('--skip_open', action='store_true',
                       help='è·³è¿‡å¼€æ”¾å¼è¯Šæ–­æµ‹è¯•')
    parser.add_argument('--skip_mcq', action='store_true',
                       help='è·³è¿‡å¤šé€‰é¢˜æµ‹è¯•')
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    config = PROJECT_CONFIG.copy()
    config.update({
        "project_dir": args.project_dir,
        "model": args.model,
        "test_size": args.test_size,
        "folk_nums": args.folk_nums
    })
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = DiagnosisArenaAutoTester(config)
    
    # è¿è¡Œæµ‹è¯•
    tester.run_all_tests()

if __name__ == "__main__":
    main()