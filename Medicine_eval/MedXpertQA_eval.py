import os
import json
import re
import subprocess
import datetime
import argparse
from pathlib import Path
from collections import defaultdict

# --- è·¯å¾„è‡ªåŠ¨æ ¡å‡† ---
BASE_DIR = Path(__file__).resolve().parent
EVAL_DIR = BASE_DIR / "MedXpertQA" / "eval"

# æµ‹è¯„æ¨¡å¼å®šä¹‰
EVAL_MODES = [
    ("zero_shot", "ao"),
    ("zero_shot", "cot")
]

def clean_r1_answer(text):
    """åŒæ­¥ eval.ipynb ä¸­çš„ R1 ç­”æ¡ˆæå–é€»è¾‘"""
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    
    # 1. å‰”é™¤æ€è€ƒè¿‡ç¨‹
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    
    # 2. å¯»æ‰¾æœ€åçš„é€‰é¡¹å­—æ¯ (A/B/C/D)
    # åŒ¹é…æ¨¡å¼ï¼šé€‰é¡¹å‰å¯èƒ½æœ‰ç©ºæ ¼ã€æ¢è¡Œæˆ–"The answer is"
    matches = re.findall(r'\b([A-D])\b', text)
    if matches:
        return matches[-1].upper()
    return ""

def calculate_detailed_metrics(result_file):
    if not result_file.exists():
        return "N/A"
    
    # ç»Ÿè®¡å­—å…¸
    stats = {
        "Overall": {"correct": 0, "total": 0},
        "Reasoning": {"correct": 0, "total": 0},
        "Understanding": {"correct": 0, "total": 0}
    }
    
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip(): continue
                data = json.loads(line)
                
                # 1. æå–é¢„æµ‹å€¼ï¼ˆå¤„ç†åˆ—è¡¨æƒ…å†µï¼‰
                pred_raw = data.get("prediction", "")
                if isinstance(pred_raw, list) and len(pred_raw) > 0:
                    pred = str(pred_raw[0]).upper() # æ‹¿åˆ° "F"
                else:
                    pred = str(pred_raw).upper()

                # 2. æå–æ ‡å‡†ç­”æ¡ˆ
                gold = data.get("label", "")
                if isinstance(gold, list) and len(gold) > 0:
                    gold = str(gold[0]).upper()
                else:
                    gold = str(gold).upper()
                
                # 3. è·å–é¢˜ç›®ç»´åº¦ (å¦‚æœå­—æ®µä¸å­˜åœ¨ï¼Œä» ID å°è¯•åˆ¤æ–­)
                q_type = data.get("question_type")
                if not q_type:
                    # æŸäº›ç‰ˆæœ¬æ ¹æ® ID å‰ç¼€åˆ¤æ–­ï¼Œè¿™é‡Œé»˜è®¤å½’ç±»åˆ° Overall
                    q_type = "Other"

                # 4. åˆ¤å®šå¯¹é”™ (ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶è‡ªå¸¦çš„ 'correct' å­—æ®µï¼Œæ›´å‡†)
                is_correct = data.get("correct")
                if is_correct is None: # å¦‚æœæ²¡è¿™ä¸ªå­—æ®µï¼Œå°±æ‰‹åŠ¨æ¯”å¯¹
                    is_correct = (pred == gold and gold != "")

                # 5. ç´¯åŠ ç»Ÿè®¡
                stats["Overall"]["total"] += 1
                if is_correct: stats["Overall"]["correct"] += 1
                
                if q_type in stats:
                    stats[q_type]["total"] += 1
                    if is_correct: stats[q_type]["correct"] += 1
        
        # æ ¼å¼åŒ–è¾“å‡º
        res_parts = []
        for cat in ["Overall", "Reasoning", "Understanding"]:
            s = stats[cat]
            if s["total"] > 0:
                acc = (s["correct"] / s["total"]) * 100
                res_parts.append(f"{cat}: {acc:.2f}%({s['correct']}/{s['total']})")
        return " | ".join(res_parts) if res_parts else "No valid data found"

    except Exception as e:
        return f"Error: {str(e)}"
    
def run_experiment(args):
    main_py = EVAL_DIR / "main.py"
    if not main_py.exists():
        print(f"âŒ è·¯å¾„é”™è¯¯: æ‰¾ä¸åˆ° {main_py}")
        return

    summary_report = []

    for method, p_type in EVAL_MODES:
        mode_name = f"{method}_{p_type}"
        print(f"\nğŸš€ [è¿è¡Œæ¨¡å¼] {mode_name}")
        
        cmd = [
            "python3", "main.py",
            "--model", args.model,
            "--dataset", args.dataset,
            "--task", args.task,
            "--method", method,
            "--prompting-type", p_type,
            "--output-dir", args.output_dir,
            "--num-threads", str(args.threads),
            "--temperature", "0.1",
            "--max-samples", str(args.max_samples),
            
        ]

        # æ‰§è¡Œ
        process = subprocess.Popen(cmd, cwd=str(EVAL_DIR), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        for line in process.stdout:
            if "Completed" in line or "INFO" in line:
                print(f"  {line.strip()}")
        process.wait()

        # åŒ¹é…è·¯å¾„ (ä½¿ç”¨ä½ æä¾›çš„å®é™…è·¯å¾„é€»è¾‘)
        result_file = (
            EVAL_DIR / "outputs" / args.output_dir / args.model / 
            args.dataset / method / p_type / f"{args.dataset}_{args.task}_output.jsonl"
        )
        
        # è·å–å¤šç»´åº¦è¯„åˆ†
        metrics_str = calculate_detailed_metrics(result_file)
        summary_report.append((mode_name, metrics_str or "N/A"))
        print(f"ğŸ {mode_name} ç»“æœ: {metrics_str}")

    # æ‰“å°æœ€ç»ˆæ€»è¡¨
    print("\n" + "="*100)
    print(f"ğŸ“Š MedXpertQA å¤šç»´åº¦è¯„æµ‹æŠ¥å‘Š (æ¨¡å‹: {args.model})")
    print("-" * 100)
    print(f"{'æµ‹è¯„æ¨¡å¼':<20} | {'å„ç»´åº¦å‡†ç¡®ç‡ (Accuracy / Correct / Total)':<60}")
    print("-" * 100)
    for mode, scores in summary_report:
        print(f"{mode:<20} | {scores}")
    print("="*100)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="deepseek-r1")
    parser.add_argument("--dataset", default="medxpertqa_sampled")
    parser.add_argument("--task", default="text")
    parser.add_argument("--output_dir", default="dev")
    parser.add_argument("--threads", default=10, type=int)
    parser.add_argument("--max-samples", default=-1, type=int)
    args = parser.parse_args()
    
    run_experiment(args)