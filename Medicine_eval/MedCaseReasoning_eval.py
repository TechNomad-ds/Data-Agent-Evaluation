import os
import subprocess
import shutil
from pathlib import Path
from openai import OpenAI
import json

# --- é…ç½®åŒºåŸŸ ---
PROJECT_NAME = "MedCaseReasoning"  # ä½ çš„é¡¹ç›®æ–‡ä»¶å¤¹åç§°
RESULT_DIR = Path("MedCaseReasoning_result")
INTERMEDIATE_DIR = RESULT_DIR / "intermediate_data"
EVALUATION_DIR = RESULT_DIR / "final_evaluation"

# API é…ç½®
API_KEY = "sk-OqIPE7A0rEMX8Rwt5NFrxB5TKAruSRGQVw7dUPRh78QpwGUi"
BASE_URL = "http://123.129.219.111:3000/v1"
MODEL_NAME = "deepseek-r1"
USER_EMAIL = "your_email@example.com" # NCBI æ¥å£éœ€è¦

def setup_directories():
    """åˆ›å»ºç»“æœè¾“å‡ºç›®å½•ç»“æ„"""
    for d in [RESULT_DIR, INTERMEDIATE_DIR, EVALUATION_DIR]:
        d.mkdir(parents=True, exist_ok=True)
    print(f"âœ… ç›®å½•ç»“æ„å·²å‡†å¤‡å°±ç»ª: {RESULT_DIR}")

def run_command(command, description):
    """æ¨¡æ‹Ÿç»ˆç«¯æ‰§è¡Œå‘½ä»¤"""
    print(f"\nğŸš€ æ­£åœ¨æ‰§è¡Œ: {description}...")
    try:
        # åœ¨é¡¹ç›®æ–‡ä»¶å¤¹å†…æ‰§è¡Œè„šæœ¬
        result = subprocess.run(
            command, 
            shell=True, 
            check=True, 
            cwd=PROJECT_NAME,
            capture_output=True, 
            text=True
        )
        print(f"âœ… å®Œæˆ: {description}")
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"âŒ å¤±è´¥: {description}\né”™è¯¯ä¿¡æ¯: {e.stderr}")
        return None

def move_intermediate_files():
    """å°†é¡¹ç›®äº§ç”Ÿçš„ä¸­é—´æ–‡ä»¶ç§»åŠ¨åˆ°ç»“æœæ–‡ä»¶å¤¹"""
    print("\nğŸ“¦ æ•´ç†ä¸­é—´äº§ç‰©...")
    # å®šä¹‰éœ€è¦ç§»åŠ¨çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆåŸºäºé¡¹ç›® README æåˆ°çš„äº§ç‰©ï¼‰
    files_to_move = [
        "case_report_pmcids.csv",
        "metadata.csv",
        "case_reports_text.parquet",
        "extracted_case_reports"
    ]
    
    project_path = Path(PROJECT_NAME)
    for item in files_to_move:
        src = project_path / item
        dst = INTERMEDIATE_DIR / item
        if src.exists():
            if dst.exists():
                if dst.is_dir(): shutil.rmtree(dst)
                else: os.remove(dst)
            shutil.move(str(src), str(dst))
            print(f"  - å·²ç§»åŠ¨: {item}")

def run_llm_inference_api():
    """ä½¿ç”¨æä¾›çš„ API æ¥å£è¿›è¡Œæœ€ç»ˆè¯„æµ‹æ¨¡æ‹Ÿ"""
    print(f"\nğŸ§  å¯åŠ¨ LLM API æ¨ç†æµ‹è¯• (æ¨¡å‹: {MODEL_NAME})...")
    
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—è¯Šæ–­åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ç—…å†è¾“å‡ºè¯Šæ–­ç»“æœã€‚"},
                {"role": "user", "content": "æ¨¡æ‹Ÿæµ‹è¯•ï¼šæ‚£è€…è¡¨ç°ä¸ºé«˜çƒ­ã€å’³å—½ã€å’³ç—°ï¼Œèƒ¸ç‰‡æ˜¾ç¤ºè‚ºéƒ¨æµ¸æ¶¦å½±ã€‚æœ€å¯èƒ½çš„è¯Šæ–­æ˜¯ï¼Ÿ"}
            ],
            max_tokens=200
        )
        
        result_content = response.choices[0].message.content
        
        # ä¿å­˜æœ€ç»ˆè¯„æµ‹ç»“æœ
        output_file = EVALUATION_DIR / "api_inference_result.json"
        result_data = {
            "model": response.model,
            "diagnosis_response": result_content,
            "usage": response.usage.dict() if response.usage else "N/A"
        }
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=4)
            
        print(f"âœ… æœ€ç»ˆè¯„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        print(f"ğŸ“¢ API å›å¤æ‘˜è¦: {result_content[:50]}...")
        
    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")

def main():
    setup_directories()
    
    # 1. ä¸‹è½½æ•°æ®
    run_command("python download_pmc.py", "ä¸‹è½½ PMC åŸå§‹æ•°æ®")
    
    # 2. è·å– ID åˆ—è¡¨
    run_command(f"python get_case_report_pmcids.py --start-date 2015/01/01 --email {USER_EMAIL}", "è·å–ç—…ä¾‹ ID")
    
    # 3. æå– XML
    run_command("python process_pmc.py", "å¹¶è¡Œæå–åŒ¹é… XML")
    
    # 4. ç”Ÿæˆå…ƒæ•°æ®ä¸æ–‡æœ¬
    run_command("python extract_metadata.py", "æå–æ–‡ç« å…ƒæ•°æ®")
    run_command("python extract_text.py", "æå–å¹¶æ¸…æ´—æ­£æ–‡æ–‡æœ¬")
    
    # 5. æ•´ç†æ–‡ä»¶
    move_intermediate_files()
    
    # 6. API è¯„æµ‹
    run_llm_inference_api()

    print("\n" + "="*30)
    print("ğŸ‰ å…¨æµç¨‹æ¨¡æ‹Ÿè¿è¡Œå®Œæˆï¼")
    print(f"ä¸­é—´äº§ç‰©è§: {INTERMEDIATE_DIR}")
    print(f"è¯„æµ‹ç»“æœè§: {EVALUATION_DIR}")
    print("="*30)

if __name__ == "__main__":
    main()