"""
Evaluation metrics for essay-style answers.
"""
import logging
import json 
import copy 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import Dict, Any, List, Optional
from rouge_score import rouge_scorer
from ..llm_clients import get_llm_response 
from ..prompts.grading_prompts import get_full_cfa_level_iii_efficient_grading_prompt
import re 

logger = logging.getLogger(__name__)

def calculate_cosine_similarity(generated_answer: str, reference_answer: str) -> Optional[float]:
    """
    Calculates the cosine similarity between a generated essay and a reference essay.

    Args:
        generated_answer: The essay answer generated by the LLM.
        reference_answer: The ground truth or model explanation.

    Returns:
        The cosine similarity score (float) between 0 and 1, or None if an error occurs
        or inputs are invalid.
    """
    if not generated_answer or not reference_answer:
        logger.warning("Cosine similarity calculation skipped: Empty generated or reference answer.")
        return None
    
    try:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([generated_answer, reference_answer])
        similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity_score)
    except Exception as e:
        logger.error(f"Error calculating cosine similarity: {e}", exc_info=True)
        return None

def calculate_rouge_l_score(generated_answer: str, reference_answer: str) -> Optional[Dict[str, float]]:
    """
    Calculates ROUGE-L (precision, recall, f1) scores between a generated essay and a reference essay.

    Args:
        generated_answer: The essay answer generated by the LLM.
        reference_answer: The ground truth or model explanation.

    Returns:
        A dictionary with 'precision', 'recall', 'f1measure' for ROUGE-L, or None if an error occurs.
    """
    if not generated_answer or not reference_answer:
        logger.warning("ROUGE-L calculation skipped: Empty generated or reference answer.")
        return None
    try:
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        scores = scorer.score(reference_answer, generated_answer) 
        rouge_l_scores = {
            "precision": scores['rougeL'].precision,
            "recall": scores['rougeL'].recall,
            "f1measure": scores['rougeL'].fmeasure
        }
        return rouge_l_scores
    except Exception as e:
        logger.error(f"Error calculating ROUGE-L score: {e}", exc_info=True)
        return None

GPT4_1_GRADER_CONFIG = {
    "config_id": "gpt-4.1-grader",
    "type": "openrouter", 
    "model_id": "gpt-4.1",
    "parameters": {
        "temperature": 0.0,
        "max_tokens": 10
    }
}

def _extract_score_from_response(response_text: str, max_score: int) -> Optional[int]:
    """Extract numerical score from GPT-4.1 response with strict validation."""
    if not response_text:
        return None
        
    response_text = response_text.strip()
    
    numbers = re.findall(r'\b(\d+)\b', response_text)
    if numbers:
        try:
            score = int(numbers[0])
            if 0 <= score <= max_score:
                return score
            else:
                logger.warning(f"Score {score} outside valid range [0, {max_score}]")
        except ValueError:
            pass
    
    logger.warning(f"Could not extract valid integer score from response: {response_text[:200]}")
    return None

def llm_unbiased_grade(question_data: Dict[str, Any], llm_answer: str) -> Dict[str, Any]:
    """
    Performs unbiased GPT-4.1 grading using CFA Level III grading criteria.

    Args:
        question_data: The original parsed question data (containing 'question', 'explanation', grading details etc.).
        llm_answer: The answer generated by the primary LLM.

    Returns:
        A dictionary containing the grading score and justification 
        (e.g., {"score": 8, "justification": "...", "error": None}).
        Returns error info if grading fails.
    """
    logger.info("Performing unbiased GPT-4.1 grading")

    original_question = question_data.get('question', '')
    vignette = question_data.get('vignette', '')
    grading_details = question_data.get('grading_details', '')
    correct_answer = question_data.get('explanation', 'See grading details for scoring criteria')
    max_score = question_data.get('max_score')

    if not all([original_question, llm_answer]):
        logger.warning("GPT-4.1 grading skipped: Missing question or LLM answer.")
        return {"score": None, "justification": "Grading skipped due to missing inputs.", "error": "Missing inputs"}

    if max_score is None:
        if grading_details:
            score_match = re.search(r"(\d+)\s*points", grading_details, re.IGNORECASE)
            if score_match:
                max_score = int(score_match.group(1))
                logger.info(f"Extracted max_score {max_score} from grading_details")
        
        if max_score is None:
            error_msg = (f"CRITICAL ERROR: max_score not found in question_data and could not be extracted "
                        f"from grading_details. This indicates a data pipeline issue where max_score from "
                        f"answer_grading_details.json is not being properly merged into the question data. "
                        f"For research accuracy, no fallback is used.")
            logger.error(error_msg)
            raise ValueError(error_msg)

    min_score = 0

    try:
        grading_prompt = get_full_cfa_level_iii_efficient_grading_prompt(
            question=original_question,
            vignette=vignette,
            answer_grading_details=grading_details,
            correct_answer=correct_answer,
            student_answer=llm_answer,
            min_score=min_score,
            max_score=max_score
        )

        grading_api_response = get_llm_response(
            prompt=grading_prompt,
            model_config=GPT4_1_GRADER_CONFIG,
            is_json_response_expected=False
        )

        if grading_api_response is None:
            logger.error("GPT-4.1 grading API call returned None.")
            return {"score": None, "justification": "Grading API call failed (returned None).", "error": "API call returned None"}

        if grading_api_response.get('error_message'):
            error_msg = grading_api_response['error_message']
            logger.error(f"GPT-4.1 grading API call failed: {error_msg}")
            return {"score": None, "justification": f"Grading API call error: {error_msg}", "error": error_msg}

        raw_response_content = grading_api_response.get('response_content')

        if raw_response_content:
            score = _extract_score_from_response(raw_response_content, max_score)
            if score is not None: 
                justification = f"Graded by GPT-4.1 using CFA Level III criteria. Score: {score}/{max_score}"
                logger.info(f"GPT-4.1 grading successful: Score {score}/{max_score}")
                return {"score": score, "justification": justification, "error": None}
            else:
                logger.warning(f"Could not extract score from GPT-4.1 response: {raw_response_content.strip()[:100]}...")
                return {"score": None, "justification": "Could not extract valid score from grading response.", "error": "Score extraction failed"}
        else:
            logger.error(f"Invalid or empty response from GPT-4.1: {grading_api_response}")
            return {"score": None, "justification": "Empty grading response from GPT-4.1.", "error": "Empty response"}

    except Exception as e:
        logger.error(f"Exception during GPT-4.1 grading: {e}", exc_info=True)
        return {"score": None, "justification": f"Exception during grading: {str(e)}", "error": str(e)}

def evaluate_essay_answers(results_data: List[Dict[str, Any]], grading_model_config: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
    """
    Evaluates a list of generated essay answers using various metrics.
    Uses cleaned answer for similarity metrics and raw answer for LLM self-grading.

    Args:
        results_data: A list of dictionaries, where each dictionary contains
                      the parsed question data (including 'explanation'), 
                      'raw_llm_answer', and 'cleaned_llm_answer'.
        grading_model_config: Optional model configuration for the LLM self-grading.
                              If None, self-grading might use a default or be skipped.

    Returns:
        The input list with added evaluation metrics.
    """
    evaluated_results = []
    for result in results_data:
        
        cleaned_generated_answer = result.get("cleaned_llm_answer") 
        raw_generated_answer = result.get("raw_llm_answer") 
        reference_answer = result.get("explanation")
        
        if cleaned_generated_answer and reference_answer and not result.get("error"):
            cosine_sim = calculate_cosine_similarity(cleaned_generated_answer, reference_answer)
            result["cosine_similarity"] = cosine_sim
            if cosine_sim is not None:
                 logger.debug(f"Cosine similarity for one item: {cosine_sim:.4f}")
            else:
                 logger.warning("Cosine similarity returned None for an item.")
        else:
            result["cosine_similarity"] = None
            logger.debug("Skipping cosine similarity due to missing cleaned answer, reference, or prior error.")
        
        if cleaned_generated_answer and reference_answer and not result.get("error"):
            rouge_l_scores = calculate_rouge_l_score(cleaned_generated_answer, reference_answer)
            if rouge_l_scores:
                result["rouge_l_precision"] = rouge_l_scores["precision"]
                result["rouge_l_recall"] = rouge_l_scores["recall"]
                result["rouge_l_f1measure"] = rouge_l_scores["f1measure"]
                logger.debug(f"ROUGE-L for one item: P={rouge_l_scores['precision']:.4f}, R={rouge_l_scores['recall']:.4f}, F1={rouge_l_scores['f1measure']:.4f}")
            else:
                result["rouge_l_precision"] = None
                result["rouge_l_recall"] = None
                result["rouge_l_f1measure"] = None
                logger.warning("ROUGE-L calculation returned None for an item.")
        else:
            result["rouge_l_precision"] = None
            result["rouge_l_recall"] = None
            result["rouge_l_f1measure"] = None
            logger.debug("Skipping ROUGE-L due to missing cleaned answer, reference, or prior error.")
        
        if raw_generated_answer and not result.get("error"):
            grade_output = llm_unbiased_grade(result, raw_generated_answer)
            result["self_grade_score"] = grade_output.get("score")
            result["self_grade_justification"] = grade_output.get("justification")
            if grade_output.get("error"):
                result["self_grade_error"] = grade_output.get("error")
            logger.debug(f"Unbiased grade output for one item: Score {result.get('self_grade_score')}")
        else:
            result["self_grade_score"] = None
            result["self_grade_justification"] = "Grading skipped (no raw answer or prior error)."
            result["self_grade_error"] = result.get("self_grade_error", "Skipped as per conditions")
            logger.debug("Skipping grading for an item.")
            
        evaluated_results.append(result)
    
    return evaluated_results 